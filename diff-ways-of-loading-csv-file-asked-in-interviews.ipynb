{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:20.768426Z","iopub.execute_input":"2025-01-30T08:40:20.768829Z","iopub.status.idle":"2025-01-30T08:40:20.778979Z","shell.execute_reply.started":"2025-01-30T08:40:20.768794Z","shell.execute_reply":"2025-01-30T08:40:20.777891Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-california-wildfire-data/POSTFIRE_MASTER_DATA_SHARE_140463065990229786.geojson\n/kaggle/input/the-california-wildfire-data/b8aeb030-140d-43d2-aa29-1a80862e3d62.csv\n/kaggle/input/customer-churn-dataset/customer_churn_dataset-testing-master.csv\n/kaggle/input/customer-churn-dataset/customer_churn_dataset-training-master.csv\n","output_type":"stream"}],"execution_count":146},{"cell_type":"code","source":"# importing modules\nimport time\nimport csv\nimport numpy as np\nimport dask.dataframe as dd\nimport sqlite3\nimport pandas as pd\nimport pyarrow.csv as pv_csv\nfrom io import StringIO\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set the seed for reproducibility\nnp.random.seed(42)  # For NumPy\nimport random\nrandom.seed(42)  # For random module\nimport torch\ntorch.manual_seed(42)  # If using PyTorch (optional)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:21.108811Z","iopub.execute_input":"2025-01-30T08:40:21.109165Z","iopub.status.idle":"2025-01-30T08:40:21.119162Z","shell.execute_reply.started":"2025-01-30T08:40:21.109138Z","shell.execute_reply":"2025-01-30T08:40:21.118128Z"}},"outputs":[{"execution_count":147,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x782734651eb0>"},"metadata":{}}],"execution_count":147},{"cell_type":"code","source":"# Path for input data\npath = \"/kaggle/input/customer-churn-dataset/customer_churn_dataset-training-master.csv\"\n\n# creating  dictionary to store all execution times\nexecution_times = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:22.752201Z","iopub.execute_input":"2025-01-30T08:40:22.752566Z","iopub.status.idle":"2025-01-30T08:40:22.756822Z","shell.execute_reply.started":"2025-01-30T08:40:22.752539Z","shell.execute_reply":"2025-01-30T08:40:22.755861Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"# A view of dataframe is only shown here as it will not be shown further in the notebook\npd.read_csv(path,low_memory=False).head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:23.327297Z","iopub.execute_input":"2025-01-30T08:40:23.327659Z","iopub.status.idle":"2025-01-30T08:40:23.960255Z","shell.execute_reply.started":"2025-01-30T08:40:23.327632Z","shell.execute_reply":"2025-01-30T08:40:23.959376Z"}},"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":"   CustomerID   Age  Gender  Tenure  Usage Frequency  Support Calls  \\\n0         2.0  30.0  Female    39.0             14.0            5.0   \n1         3.0  65.0  Female    49.0              1.0           10.0   \n\n   Payment Delay Subscription Type Contract Length  Total Spend  \\\n0           18.0          Standard          Annual        932.0   \n1            8.0             Basic         Monthly        557.0   \n\n   Last Interaction  Churn  \n0              17.0    1.0  \n1               6.0    1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerID</th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Tenure</th>\n      <th>Usage Frequency</th>\n      <th>Support Calls</th>\n      <th>Payment Delay</th>\n      <th>Subscription Type</th>\n      <th>Contract Length</th>\n      <th>Total Spend</th>\n      <th>Last Interaction</th>\n      <th>Churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.0</td>\n      <td>30.0</td>\n      <td>Female</td>\n      <td>39.0</td>\n      <td>14.0</td>\n      <td>5.0</td>\n      <td>18.0</td>\n      <td>Standard</td>\n      <td>Annual</td>\n      <td>932.0</td>\n      <td>17.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.0</td>\n      <td>65.0</td>\n      <td>Female</td>\n      <td>49.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>8.0</td>\n      <td>Basic</td>\n      <td>Monthly</td>\n      <td>557.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":149},{"cell_type":"markdown","source":"Note: Please run each execution twice to see the exact time as some modules take high time to run once and then the time is significantly reduced ","metadata":{}},{"cell_type":"markdown","source":"## 1. Using the csv Module (Built-in)","metadata":{}},{"cell_type":"code","source":"# Using csv.reader\nstart = time.time()\nwith open(path, 'r') as file:\n    reader = csv.reader(file)\n    data_csv_reader = list(reader)  # Read all rows into a list\nexecution_times[\"csv_reader\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:25.448006Z","iopub.execute_input":"2025-01-30T08:40:25.448342Z","iopub.status.idle":"2025-01-30T08:40:27.859576Z","shell.execute_reply.started":"2025-01-30T08:40:25.448316Z","shell.execute_reply":"2025-01-30T08:40:27.858605Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"# Using csv.DictReader\nstart = time.time()\nwith open(path, 'r') as file:\n    reader = csv.DictReader(file)\n    data_csv_dictreader = [row for row in reader]  # Convert to a list of dictionaries\nexecution_times[\"csv_dictreader\"] = time.time() - start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:27.860957Z","iopub.execute_input":"2025-01-30T08:40:27.861289Z","iopub.status.idle":"2025-01-30T08:40:29.668761Z","shell.execute_reply.started":"2025-01-30T08:40:27.861257Z","shell.execute_reply":"2025-01-30T08:40:29.667677Z"}},"outputs":[],"execution_count":151},{"cell_type":"markdown","source":"## 2. Using pandas Library","metadata":{}},{"cell_type":"code","source":"start = time.time()\ndd = pd.read_csv(path,low_memory=False)\nexecution_times[\"pandas\"] = time.time() - start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:29.670383Z","iopub.execute_input":"2025-01-30T08:40:29.670695Z","iopub.status.idle":"2025-01-30T08:40:30.203099Z","shell.execute_reply.started":"2025-01-30T08:40:29.670669Z","shell.execute_reply":"2025-01-30T08:40:30.201964Z"}},"outputs":[],"execution_count":152},{"cell_type":"markdown","source":"## 3. csv.reader with io.StringIO","metadata":{}},{"cell_type":"code","source":"# Using csv.reader with io.StringIO\nstart = time.time()\nfile = StringIO(path)  # Convert string to file-like object\nreader = csv.reader(file)\ndata_csv_stringio = list(reader)  # Read all rows into a list\nexecution_times[\"csv_reader_StringIO\"] = time.time() - start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:30.204603Z","iopub.execute_input":"2025-01-30T08:40:30.204938Z","iopub.status.idle":"2025-01-30T08:40:30.209861Z","shell.execute_reply.started":"2025-01-30T08:40:30.204910Z","shell.execute_reply":"2025-01-30T08:40:30.208856Z"}},"outputs":[],"execution_count":153},{"cell_type":"markdown","source":"## Using sqlite3 (For Loading into a Database)","metadata":{}},{"cell_type":"code","source":"\n# Step 1: Create an SQLite database and insert the CSV data\nconn = sqlite3.connect(':memory:')  # Use in-memory database for simplicity\ncursor = conn.cursor()\n\n# Read the CSV file into a pandas DataFrame\ndf_csv = pd.read_csv(path)\n\n# Creating a table dynamically based on the CSV columns\ncolumns = ', '.join([f\"{col} TEXT\" for col in df_csv.columns])  # Define columns as TEXT type\ncursor.execute(f'''CREATE TABLE data ({columns})''')\n\n# Insert data into the table\ndf_csv.to_sql('data', conn, if_exists='replace', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:30.210781Z","iopub.execute_input":"2025-01-30T08:40:30.211081Z","iopub.status.idle":"2025-01-30T08:40:32.149596Z","shell.execute_reply.started":"2025-01-30T08:40:30.211058Z","shell.execute_reply":"2025-01-30T08:40:32.148629Z"}},"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"440833"},"metadata":{}}],"execution_count":154},{"cell_type":"code","source":"# Step 2: Measuring execution time for loading data from SQLite\nstart = time.time()\n# Query data from SQLite\ndf_sqlite = pd.read_sql_query(\"SELECT * FROM data\", conn)\nexecution_times[\"sqlite3\"] = time.time() - start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:32.150577Z","iopub.execute_input":"2025-01-30T08:40:32.150913Z","iopub.status.idle":"2025-01-30T08:40:34.111063Z","shell.execute_reply.started":"2025-01-30T08:40:32.150888Z","shell.execute_reply":"2025-01-30T08:40:34.110083Z"}},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":"## Using dask for Large CSV Files","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\n# Step 1: Measuring execution time for loading the data with Dask\nstart = time.time()\n\n# Read the CSV file with Dask (this loads the data lazily)\ndf_dask = dd.read_csv(path,assume_missing=True)\n\n# Triggering computation by forcing Dask to load the data into memory\ndf_dask.compute()\n\nexecution_times[\"dask\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:34.113328Z","iopub.execute_input":"2025-01-30T08:40:34.113639Z","iopub.status.idle":"2025-01-30T08:40:34.659474Z","shell.execute_reply.started":"2025-01-30T08:40:34.113611Z","shell.execute_reply":"2025-01-30T08:40:34.658222Z"}},"outputs":[],"execution_count":156},{"cell_type":"markdown","source":"## Using pyarrow for High-Performance Data Loading","metadata":{}},{"cell_type":"code","source":"# Step 1: Measure execution time for loading the data with PyArrow\nstart = time.time()\n\n# Read the CSV file using PyArrow\ntable = pv_csv.read_csv(path)\n\n# Convert to pandas DataFrame (if needed)\ndf_pyarrow = table.to_pandas()\n\nexecution_times[\"pyarrow\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:34.661218Z","iopub.execute_input":"2025-01-30T08:40:34.661592Z","iopub.status.idle":"2025-01-30T08:40:34.952439Z","shell.execute_reply.started":"2025-01-30T08:40:34.661555Z","shell.execute_reply":"2025-01-30T08:40:34.951612Z"}},"outputs":[],"execution_count":157},{"cell_type":"markdown","source":"## Using modin for Parallel Loading","metadata":{}},{"cell_type":"code","source":"# !pip install modin -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:34.953287Z","iopub.execute_input":"2025-01-30T08:40:34.953587Z","iopub.status.idle":"2025-01-30T08:40:34.957558Z","shell.execute_reply.started":"2025-01-30T08:40:34.953565Z","shell.execute_reply":"2025-01-30T08:40:34.956539Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"import modin.pandas as mpd\n\n# Step 1: Measure execution time for loading the data with Modin\nstart = time.time()\n\n# Read the CSV file using Modin (which uses pandas API)\ndf_modin = mpd.read_csv(path)\n\nexecution_times[\"modin\"] = time.time() - start","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:34.958668Z","iopub.execute_input":"2025-01-30T08:40:34.958986Z","iopub.status.idle":"2025-01-30T08:40:35.375115Z","shell.execute_reply.started":"2025-01-30T08:40:34.958952Z","shell.execute_reply":"2025-01-30T08:40:35.374049Z"}},"outputs":[],"execution_count":159},{"cell_type":"markdown","source":"## Using petl for Transformations","metadata":{}},{"cell_type":"code","source":"# Install petl module\n# !pip install petl -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:35.376192Z","iopub.execute_input":"2025-01-30T08:40:35.376557Z","iopub.status.idle":"2025-01-30T08:40:35.380193Z","shell.execute_reply.started":"2025-01-30T08:40:35.376511Z","shell.execute_reply":"2025-01-30T08:40:35.379180Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"import petl as etl\n\n# Step 1: Measure execution time for loading the data with Petl\nstart = time.time()\n# Read the CSV file using Petl\ntable = etl.fromcsv(path)\nexecution_times[\"petl\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:35.381147Z","iopub.execute_input":"2025-01-30T08:40:35.381423Z","iopub.status.idle":"2025-01-30T08:40:35.400387Z","shell.execute_reply.started":"2025-01-30T08:40:35.381401Z","shell.execute_reply":"2025-01-30T08:40:35.399156Z"}},"outputs":[],"execution_count":161},{"cell_type":"markdown","source":"## Using pyspark for Big Data","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\n# Initialize SparkSession\nspark = SparkSession.builder.appName(\"BigDataCSVProcessing\").getOrCreate()\n\n# Step 1: Measure execution time for loading the data with PySpark\nstart = time.time()\n\n# Load the CSV file using PySpark (returns a DataFrame)\ndf_spark = spark.read.csv(path, header=True, inferSchema=True)\nexecution_times[\"pyspark\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:35.401496Z","iopub.execute_input":"2025-01-30T08:40:35.401836Z","iopub.status.idle":"2025-01-30T08:40:36.748461Z","shell.execute_reply.started":"2025-01-30T08:40:35.401804Z","shell.execute_reply":"2025-01-30T08:40:36.747338Z"}},"outputs":[],"execution_count":162},{"cell_type":"markdown","source":"## Using Polars","metadata":{}},{"cell_type":"code","source":"# !pip install polars -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:36.751229Z","iopub.execute_input":"2025-01-30T08:40:36.751525Z","iopub.status.idle":"2025-01-30T08:40:36.755442Z","shell.execute_reply.started":"2025-01-30T08:40:36.751501Z","shell.execute_reply":"2025-01-30T08:40:36.754275Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"import polars as pl\n\n# Step 1: Measure execution time for loading the data with Polars\nstart = time.time()\n\n# Read the CSV file using Polars\ndf_polars = pl.read_csv(path, ignore_errors=True)\n\nexecution_times[\"polars\"] = time.time() - start\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:36.756724Z","iopub.execute_input":"2025-01-30T08:40:36.757173Z","iopub.status.idle":"2025-01-30T08:40:36.901639Z","shell.execute_reply.started":"2025-01-30T08:40:36.757134Z","shell.execute_reply":"2025-01-30T08:40:36.900743Z"}},"outputs":[],"execution_count":164},{"cell_type":"markdown","source":"## Using DuckDB","metadata":{}},{"cell_type":"code","source":"import duckdb\n\n# Step 1: Measure execution time for loading the data with DuckDB\nstart = time.time()\n\n# Open a DuckDB connection and read the CSV file into a DuckDB relation (SQL table-like structure)\nconn = duckdb.connect()\ndf_duckdb = conn.execute(f\"SELECT * FROM read_csv_auto('{path}')\").fetchdf()\n\nexecution_times[\"duckdb\"] = time.time() - start\n\n# Close the DuckDB connection\nconn.close()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:37.597152Z","iopub.execute_input":"2025-01-30T08:40:37.597514Z","iopub.status.idle":"2025-01-30T08:40:38.270803Z","shell.execute_reply.started":"2025-01-30T08:40:37.597482Z","shell.execute_reply":"2025-01-30T08:40:38.269871Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"df_comparison = pd.DataFrame(list(execution_times.items()), columns=[\"Method\", \"Execution Time (s)\"])\ndf_comparison[\"Execution Time (s)\"] = df_comparison[\"Execution Time (s)\"].apply(lambda x: f\"{x:.3e}\")  # Convert to scientific notation\n\n# Display results\ndf_comparison","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:40:15.833678Z","iopub.execute_input":"2025-01-30T08:40:15.834098Z","iopub.status.idle":"2025-01-30T08:40:15.845197Z","shell.execute_reply.started":"2025-01-30T08:40:15.834065Z","shell.execute_reply":"2025-01-30T08:40:15.844036Z"}},"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"                 Method Execution Time (s)\n0            csv_reader          1.648e+00\n1        csv_dictreader          1.847e+00\n2                pandas          5.103e-01\n3   csv_reader_StringIO          1.643e-04\n4               sqlite3          2.081e+00\n5                  dask          5.484e-01\n6               pyarrow          2.936e-01\n7                 modin          4.102e-01\n8                  petl          1.388e-04\n9               pyspark          1.194e+00\n10               polars          1.128e-01\n11               duckdb          7.838e-01","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>csv_reader</td>\n      <td>1.648e+00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>csv_dictreader</td>\n      <td>1.847e+00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pandas</td>\n      <td>5.103e-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>csv_reader_StringIO</td>\n      <td>1.643e-04</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sqlite3</td>\n      <td>2.081e+00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>dask</td>\n      <td>5.484e-01</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>pyarrow</td>\n      <td>2.936e-01</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>modin</td>\n      <td>4.102e-01</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>petl</td>\n      <td>1.388e-04</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>pyspark</td>\n      <td>1.194e+00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>polars</td>\n      <td>1.128e-01</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>duckdb</td>\n      <td>7.838e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":145},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}